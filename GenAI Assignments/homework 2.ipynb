{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fafd0ec1-9e24-4952-9076-5141ac5461dd",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bdb477d62289a48c2eb4ddf4971dc481",
     "grade": false,
     "grade_id": "cell-39a7bc1e018ef81b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 2023.11.09 - Introduction to Transformers Continued |Â Homework 2\n",
    "In this exercise, you will implement a masked scaled dot-product attention, which allows the model to focus on different parts of the input sequence.\n",
    "\n",
    "Base your code on the following skeleton code that we provide:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d580381d-cfe3-4249-9a72-1c351262e27d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "06fb7da52413c3d6c4c7cb4e2a3a369d",
     "grade": false,
     "grade_id": "cell-c9ecc6f381f97979",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Exercise 1 - Scaled Dot-Product Attention:\n",
    "Implement the scaled dot-product attention function, which is defined as `Attention(Q, K, V) = softmax(QK^T / sqrt(d_k))V`, where Q, K, V are queries, keys, and values, respectively, and `d_k` is the dimensionality of the keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3aa53ff9-d1da-4bd0-9ffc-9e344de39dbd",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ee31be9372765d6cb5975becadea05d5",
     "grade": false,
     "grade_id": "cell-25baa944006914f3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4dcbe1b-6c62-4017-8ba7-2958bf479d6d",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3680100ad8144a739bd7c471be49e81e",
     "grade": false,
     "grade_id": "cell-67c6b04861004718",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "revert": "# Import any additional dependencies\n# YOUR CODE HERE\nraise NotImplementedError()"
   },
   "outputs": [],
   "source": [
    "# Import any additional dependencies\n",
    "# YOUR CODE HERE\n",
    "# raise NotImplementedError()\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d665f449-7358-4193-8e54-6ab79a625a2a",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2dfba6e3715a6472cd7b60ce256851e8",
     "grade": false,
     "grade_id": "cell-30f1c24f1dc6b774",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "revert": "def scaled_dot_product_attention(Q: Tensor, K: Tensor, V: Tensor) -> (Tensor, Tensor):\n    \"\"\"\n    Computes the scaled dot-product attention.\n    The function takes three inputs: Q (query), K (key), V (value).\n    It computes the dot products of the query with all keys, divides each by sqrt(d_k), and applies a softmax function to obtain the weights on the values.\n\n    Parameters:\n    - Q: Tensor of shape (batch_size, block_size, d_k)\n    - K: Tensor of shape (batch_size, block_size, d_k)\n    - V: Tensor of shape (batch_size, block_size, d_k)\n\n    Returns:\n    - output: Tensor of shape (batch_size, block_size, d_k)\n        The resulting tensor after applying the attention mechanism.\n    - attention_weights: Tensor of shape (batch_size, block_size, block_size)\n        The attention weights after applying softmax.\n    \"\"\"\n    # YOUR CODE HERE\n    raise NotImplementedError()"
   },
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(Q: Tensor, K: Tensor, V: Tensor) -> (Tensor, Tensor):\n",
    "    \"\"\"\n",
    "    Computes the scaled dot-product attention.\n",
    "    The function takes three inputs: Q (query), K (key), V (value).\n",
    "    It computes the dot products of the query with all keys, divides each by sqrt(d_k), and applies a softmax function to obtain the weights on the values.\n",
    "\n",
    "    Parameters:\n",
    "    - Q: Tensor of shape (batch_size, block_size, d_k)\n",
    "    - K: Tensor of shape (batch_size, block_size, d_k)\n",
    "    - V: Tensor of shape (batch_size, block_size, d_k)\n",
    "\n",
    "    Returns:\n",
    "    - output: Tensor of shape (batch_size, block_size, d_k)\n",
    "        The resulting tensor after applying the attention mechanism.\n",
    "    - attention_weights: Tensor of shape (batch_size, block_size, block_size)\n",
    "        The attention weights after applying softmax.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    # raise NotImplementedError()\n",
    "\n",
    "    # Calculate the dot product of Query and Key\n",
    "    dot_product = torch.bmm(Q, K.transpose(1, 2))\n",
    "\n",
    "    # Scale the dot product by the square root of the dimension of Key\n",
    "    d_k = K.size(-1)\n",
    "    scaled_dot_product = dot_product / torch.sqrt(torch.tensor(d_k, dtype=torch.float32))\n",
    "\n",
    "    # Apply softmax to obtain the attention weights\n",
    "    attention_weights = F.softmax(scaled_dot_product, dim=-1)\n",
    "\n",
    "    # Apply the attention weights to the values\n",
    "    output = torch.bmm(attention_weights, V)\n",
    "\n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d2fe14-e9d8-4f53-8294-c3abaa9c0e1f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7f2f07e862b0cf61e03c18790034c945",
     "grade": false,
     "grade_id": "cell-010a93a077c6e054",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Run Exercise 1\n",
    "Run this cell to evaluate your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96b2b6dc-fb00-466c-9737-9be8bcb90a40",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8a56e4578b0b279f57c2f02fdea291f5",
     "grade": true,
     "grade_id": "cell-8d051d2e6d6b82e5",
     "locked": true,
     "points": 0.75,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============ Scaled dot-product attention\n",
      "Attention Output:\n",
      "tensor([[[ 0.2957,  0.3332,  0.0674,  0.2081],\n",
      "         [ 0.2628,  0.4036,  0.0423,  0.2148],\n",
      "         [ 0.3083,  0.4486,  0.2576,  0.2905]],\n",
      "\n",
      "        [[-0.1859, -0.8284, -0.5916, -1.0532],\n",
      "         [-0.1266, -0.8076, -0.5838, -1.1651],\n",
      "         [ 0.1222, -0.4625, -0.4709, -0.1724]]])\n",
      "\n",
      "Attention Weights:\n",
      "tensor([[[0.4537, 0.3877, 0.1586],\n",
      "         [0.4461, 0.3433, 0.2105],\n",
      "         [0.2678, 0.3638, 0.3684]],\n",
      "\n",
      "        [[0.5597, 0.1483, 0.2920],\n",
      "         [0.5515, 0.0702, 0.3784],\n",
      "         [0.1524, 0.7299, 0.1178]]])\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Expected Output:\n",
    "============ Scaled dot-product attention\n",
    "Attention Output:\n",
    "tensor([[[ 0.2957,  0.3332,  0.0674,  0.2081], # these numbers will be different, but the dimensions should be the same\n",
    "         [ 0.2628,  0.4036,  0.0423,  0.2148],\n",
    "         [ 0.3083,  0.4486,  0.2576,  0.2905]],\n",
    "\n",
    "        [[-0.1859, -0.8284, -0.5916, -1.0532],\n",
    "         [-0.1266, -0.8076, -0.5838, -1.1651],\n",
    "         [ 0.1222, -0.4625, -0.4709, -0.1724]]])\n",
    "\n",
    "Attention Weights:\n",
    "tensor([[[0.4537, 0.3877, 0.1586],\n",
    "         [0.4461, 0.3433, 0.2105],\n",
    "         [0.2678, 0.3638, 0.3684]],\n",
    "\n",
    "        [[0.5597, 0.1483, 0.2920],\n",
    "         [0.5515, 0.0702, 0.3784],\n",
    "         [0.1524, 0.7299, 0.1178]]])\n",
    "\"\"\"\n",
    "print(\"============ Scaled dot-product attention\")\n",
    "torch.manual_seed(0)      # to keep results consistent between runs\n",
    "Q = torch.randn(2, 3, 4)  # batch size of 2, block size of 3, and d_k of 4\n",
    "K = torch.randn(2, 3, 4)  # batch size of 2, block size of 3, and d_k of 4\n",
    "V = torch.randn(2, 3, 4)  # batch size of 2, block size of 3, and d_k of 4\n",
    "\n",
    "output, weights = scaled_dot_product_attention(Q, K, V)\n",
    "print(f\"Attention Output:\\n{output}\\n\")\n",
    "print(f\"Attention Weights:\\n{weights}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c8cc15-a0a0-4e07-91ef-41bbf322a499",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "aff7ccf2691f63d53f1b7fc040cd04db",
     "grade": false,
     "grade_id": "cell-148c87a0484aa1bf",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Excercise 2 -  Add a mask to the attention mechanism:\n",
    "The next step is to add a masking functionality to the attention mechanism.\n",
    "A mask allows the model to selectively ignore certain positions within the sequence, used to handle variable sequence lengths or to prevent the model from attending to future positions in the a decoder model.\n",
    "\n",
    "\n",
    "_Note:_\n",
    "The mask should have the same batch size and block size as the queries and should contain ones for positions that should be attended to and zeros for masked positions. Before the softmax step, masked positions are filled with large negative values (e.g. -torch.inf), effectively removing these positions from consideration in the attention weights.\n",
    "\n",
    "Feel free to copy & paste your implementation from Excercise 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "694e2bad-d14e-4e14-8d12-0d92ce306df5",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7b479933685ca0601c81fab32fe544f6",
     "grade": false,
     "grade_id": "cell-310dc2fcffbe389d",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "revert": "def scaled_dot_product_attention(Q: Tensor, K: Tensor, V: Tensor, mask: Tensor=None) -> (Tensor, Tensor):\n    \"\"\"\n    Computes the scaled dot-product attention.\n    The function takes three inputs: Q (query), K (key), V (value).\n    It computes the dot products of the query with all keys, divides each by sqrt(d_k), and applies a softmax function to obtain the weights on the values.\n\n    Parameters:\n    - Q: Tensor of shape (batch_size, block_size, d_k)\n    - K: Tensor of shape (batch_size, block_size, d_k)\n    - V: Tensor of shape (batch_size, block_size, d_k)\n    - mask: Optional tensor of shape (batch_size, 1, block_size).\n            The mask should contain 1s for positions to attend to and 0s for masked positions.\n            Masked positions are filled with large negative values before the softmax step, effectively excluding them from consideration in the attention weights.\n\n    Returns:\n    - output: Tensor of shape (batch_size, block_size, d_k)\n        The resulting tensor after applying the attention mechanism.\n    - attention_weights: Tensor of shape (batch_size, block_size, block_size)\n        The attention weights after applying softmax.\n    \"\"\"\n    # YOUR CODE HERE\n    raise NotImplementedError()"
   },
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(Q: Tensor, K: Tensor, V: Tensor, mask: Tensor=None) -> (Tensor, Tensor):\n",
    "    \"\"\"\n",
    "    Computes the scaled dot-product attention.\n",
    "    The function takes three inputs: Q (query), K (key), V (value).\n",
    "    It computes the dot products of the query with all keys, divides each by sqrt(d_k), and applies a softmax function to obtain the weights on the values.\n",
    "\n",
    "    Parameters:\n",
    "    - Q: Tensor of shape (batch_size, block_size, d_k)\n",
    "    - K: Tensor of shape (batch_size, block_size, d_k)\n",
    "    - V: Tensor of shape (batch_size, block_size, d_k)\n",
    "    - mask: Optional tensor of shape (batch_size, 1, block_size).\n",
    "            The mask should contain 1s for positions to attend to and 0s for masked positions.\n",
    "            Masked positions are filled with large negative values before the softmax step, effectively excluding them from consideration in the attention weights.\n",
    "\n",
    "    Returns:\n",
    "    - output: Tensor of shape (batch_size, block_size, d_k)\n",
    "        The resulting tensor after applying the attention mechanism.\n",
    "    - attention_weights: Tensor of shape (batch_size, block_size, block_size)\n",
    "        The attention weights after applying softmax.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    # The procedure, from the previous task, is repeated\n",
    "    # This time the mask is applied\n",
    "    \n",
    "    # Calculate the dot product of Query and Key\n",
    "    dot_product = torch.bmm(Q, K.transpose(1, 2))\n",
    "\n",
    "    # Scale the dot product by the square root of the dimension of Key\n",
    "    d_k = K.size(-1)\n",
    "    scaled_dot_product = dot_product / torch.sqrt(torch.tensor(d_k, dtype=torch.float32))\n",
    "\n",
    "    # Apply the mask \n",
    "    if mask is not None:\n",
    "        scaled_dot_product.masked_fill_(mask == 0, float('-inf'))\n",
    "\n",
    "    # Apply softmax to obtain the attention weights\n",
    "    attention_weights = F.softmax(scaled_dot_product, dim=-1)\n",
    "\n",
    "    # Apply the attention weights to the values\n",
    "    output = torch.bmm(attention_weights, V)\n",
    "\n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075d8542-c53c-4caa-94f0-c3cc97bed769",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c0454debf52cddb93025c9e07c54ada1",
     "grade": false,
     "grade_id": "cell-7bc601c2f2a72340",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Run Exercise 2\n",
    "Run these cells to evaluate your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8980703e-b5ca-4966-aa8f-ee9378a40042",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c612a51400c5f7f2045541adc68ef412",
     "grade": true,
     "grade_id": "cell-ed04181ba2768b2e",
     "locked": true,
     "points": 0.25,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============ Masked scaled dot-product attention\n",
      "Attention Output:\n",
      "tensor([[[ 0.2957,  0.3332,  0.0674,  0.2081],\n",
      "         [ 0.2628,  0.4036,  0.0423,  0.2148],\n",
      "         [ 0.3083,  0.4486,  0.2576,  0.2905]],\n",
      "\n",
      "        [[-0.5912, -1.0937, -0.6829, -0.9884],\n",
      "         [-0.6886, -1.1866, -0.7139, -1.1376],\n",
      "         [ 0.0322, -0.4995, -0.4843, -0.0339]]])\n",
      "\n",
      "Attention Weights:\n",
      "tensor([[[0.4537, 0.3877, 0.1586],\n",
      "         [0.4461, 0.3433, 0.2105],\n",
      "         [0.2678, 0.3638, 0.3684]],\n",
      "\n",
      "        [[0.7905, 0.2095, 0.0000],\n",
      "         [0.8871, 0.1129, 0.0000],\n",
      "         [0.1727, 0.8273, 0.0000]]])\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Expected Output:\n",
    "============ Masked scaled dot-product attention\n",
    "Attention Output:\n",
    "tensor([[[ 0.2957,  0.3332,  0.0674,  0.2081],\n",
    "         [ 0.2628,  0.4036,  0.0423,  0.2148],\n",
    "         [ 0.3083,  0.4486,  0.2576,  0.2905]],\n",
    "\n",
    "        [[-0.5912, -1.0937, -0.6829, -0.9884],\n",
    "         [-0.6886, -1.1866, -0.7139, -1.1376],\n",
    "         [ 0.0322, -0.4995, -0.4843, -0.0339]]])\n",
    "\n",
    "Attention Weights:\n",
    "tensor([[[0.4537, 0.3877, 0.1586],\n",
    "         [0.4461, 0.3433, 0.2105],\n",
    "         [0.2678, 0.3638, 0.3684]],\n",
    "\n",
    "        [[0.7905, 0.2095, 0.0000],\n",
    "         [0.8871, 0.1129, 0.0000],\n",
    "         [0.1727, 0.8273, 0.0000]]])\n",
    "\"\"\"\n",
    "print(\"============ Masked scaled dot-product attention\")\n",
    "torch.manual_seed(0)      # to keep results consistent between runs\n",
    "Q = torch.randn(2, 3, 4)  # batch size of 2, block size of 3, and d_k of 4\n",
    "K = torch.randn(2, 3, 4)  # batch size of 2, block size of 3, and d_k of 4\n",
    "V = torch.randn(2, 3, 4)  # batch size of 2, block size of 3, and d_k of 4\n",
    "\n",
    "mask = torch.ones_like(Q[:, :, 0]).unsqueeze(1)  # Mask of shape (batch size, 1, block size)\n",
    "mask[1, 0, 2:] = 0 # Mask last token of the second block\n",
    "\n",
    "output, weights = scaled_dot_product_attention(Q, K, V, mask)\n",
    "print(f\"Attention Output:\\n{output}\\n\")\n",
    "print(f\"Attention Weights:\\n{weights}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843fe242-5284-4042-af95-d347213a4531",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python with Torch",
   "language": "python",
   "name": "python3_2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
