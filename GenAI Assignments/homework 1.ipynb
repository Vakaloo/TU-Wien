{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fea1d0e2-9065-4a73-b80f-7f3582c9f158",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e13f9e4e93a9ec793f69499d201d2130",
     "grade": false,
     "grade_id": "cell-5682c133e759dc27",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### 2023.10.19 - Introduction to Transformers |Â Homework 1\n",
    "In this exercise, you will implement your own character-based Tokenizer as well as an Embedding Layer from scratch.\\\n",
    "Base your code on the following skeleton code that we provide:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63053078-1107-4b5f-b137-3d91e623588a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ce13e3e0e5cf3995128aef711b5d3f6b",
     "grade": false,
     "grade_id": "cell-5cb7bddef27aecbc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Exercise 1 - Character-based Tokenizer:\n",
    "- Initialize your vocabulary with a list of unique characters. Consider alphabetic letters, common punctuation and numbers for a start. Your initial vocabulary should at least include lowercase English letters (a-z), digits (0-9), and common punctuation marks (e.g., ., !, ?).\n",
    "\n",
    "- Implement a basic character-based tokenizer. Ensure to include a special $<UNK>$ (\"unknown\") token to handle characters outside your vocabulary.\n",
    "  - The tokenizer should be capable of:\n",
    "    - Parsing a string into a list of characters.\n",
    "    - Encoding a list of characters into their corresponding indices in the vocabulary.\n",
    "    - Decoding a list of indices back into a string.\n",
    "    - When encoding, return the token ID for $<UNK>$ for any character not in the vocabulary. Similarly, when decoding, return the $<UNK>$ token for any unknown token ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1517ab3c-cadf-4111-bcca-b2382158919b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cf625438ea155011c12d957e574e533b",
     "grade": false,
     "grade_id": "cell-d8933b632fbfd372",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d0d7b4d3-a08b-47df-9d6c-72b39b928cfc",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "347792a63cf50d8f33e333c8051d05bb",
     "grade": false,
     "grade_id": "cell-4b8f6c67cd37f8ea",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "revert": "# define your vocab here e.g. vocab = list('')\n# YOUR CODE HERE\nraise NotImplementedError()"
   },
   "outputs": [],
   "source": [
    "# define your vocab here e.g. vocab = list('')\n",
    "# YOUR CODE HERE\n",
    "vocab = list('abcdefghijklmnopqrstuvwxyz0123456789.,!?') # define your vocabulary here\n",
    "vocab.append('<UNK>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ad7ed380-7dc2-4432-b9eb-790b89f3a891",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f67d2e1434a61f0b36d262e014eaf2d1",
     "grade": false,
     "grade_id": "cell-e29e3ca6d162432f",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "revert": "class Tokenizer:\n    def __init__(self, vocab: List[str]):\n        # Add <UNK> token if it's not already in the vocabulary\n        # YOUR CODE HERE\n        raise NotImplementedError()\n        \n\n    def parse(self, input: str) -> List[str]:\n        \"\"\"Convert a string to a list of characters.\"\"\"\n        # YOUR CODE HERE\n        raise NotImplementedError()\n\n    def encode(self, tokens: List[str]) -> List[int]:\n        \"\"\" Encode a list of tokens into their corresponding indices.\"\"\"\n        # YOUR CODE HERE\n        raise NotImplementedError()\n\n    def decode(self, indices: List[int]) -> str:\n        \"\"\"Decode a list of indices back into a string.\"\"\"\n        # YOUR CODE HERE\n        raise NotImplementedError()"
   },
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    def __init__(self, vocab: List[str]):\n",
    "        # Add <UNK> token if it's not already in the vocabulary\n",
    "        # YOUR CODE HERE\n",
    "        # raise NotImplementedError()\n",
    "        self.vocab = vocab\n",
    "\n",
    "    def parse(self, input: str) -> List[str]:\n",
    "        \"\"\"Convert a string to a list of characters.\"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        # raise NotImplementedError()\n",
    "        return list(input)\n",
    "\n",
    "    def encode(self, tokens: List[str]) -> List[int]:\n",
    "        \"\"\" Encode a list of tokens into their corresponding indices.\"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        # raise NotImplementedError()\n",
    "        encoded_indices = [vocab.index(token) if token in vocab else vocab.index('<UNK>') for token in tokens]\n",
    "        return encoded_indices\n",
    "\n",
    "    def decode(self, indices: List[int]) -> str:\n",
    "        \"\"\"Decode a list of indices back into a string.\"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        # raise NotImplementedError()\n",
    "        decode_list = []\n",
    "        for i in indices:\n",
    "            try:\n",
    "                decode_list.append(vocab[i])\n",
    "            except:\n",
    "                decode_list.append('<UNK>')\n",
    "        decode_string = \"\".join(decode_list)\n",
    "        return decode_string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05fab628-2415-4a56-8112-ae4f32c840e6",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2a643b6fd02874671960ba78f32f905a",
     "grade": false,
     "grade_id": "cell-f9201057861388b1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Run Exercise 1\n",
    "Run this cell to evaluate your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "52565c1d-f6a9-4139-85b5-5a3624d1ad4a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7568761825d5c60ee8c41e5fe88e98af",
     "grade": true,
     "grade_id": "cell-84e3da6eead305ce",
     "locked": true,
     "points": 0.25,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============ Tokenizer\n",
      "tokenizer.parse: ['c', 'a', 't', 'e', 'r', 'p', 'i', 'l', 'l', 'a', 'r', '!']\n",
      "tokenizer.encode: [2, 0, 19, 4, 17, 15, 8, 11, 11, 0, 17, 38]\n",
      "tokenizer.decode: caterpillar!\n",
      "tokenizer.encode/decode unknown: <UNK>\n",
      "tokenizer.decode out of bounds: <UNK>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Expected Output:\n",
    "============ Tokenizer\n",
    "tokenizer.parse: ['c', 'a', 't', 'e', 'r', 'p', 'i', 'l', 'l', 'a', 'r', '!']\n",
    "tokenizer.encode: [28, 26, 45, 30, 43, 41, 34, 37, 37, 26, 43, 53] # these numbers will be different\n",
    "tokenizer.decode: caterpillar!\n",
    "tokenizer.encode/decode unknown: <UNK> # This will be different if you choose to use a different <UNK> token\n",
    "tokenizer.decode out of bounds: <UNK>  # This will be different if you choose to use a different <UNK> token\n",
    "\"\"\"\n",
    "\n",
    "tokenizer = Tokenizer(vocab)\n",
    "\n",
    "print(\"============ Tokenizer\")\n",
    "# Test parsing\n",
    "tokens = tokenizer.parse('caterpillar!')\n",
    "print(f\"tokenizer.parse: {tokens}\")\n",
    "\n",
    "# Test encoding\n",
    "token_ids = tokenizer.encode(tokens)\n",
    "print(f\"tokenizer.encode: {token_ids}\")\n",
    "\n",
    "# Test decoding\n",
    "print(f\"tokenizer.decode: {tokenizer.decode(token_ids)}\")\n",
    "\n",
    "# Test <UNK>\n",
    "print(f\"tokenizer.encode/decode unknown: {tokenizer.decode(tokenizer.encode(['$']))}\")\n",
    "print(f\"tokenizer.decode out of bounds: {tokenizer.decode([100])}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8e72cf-8ba7-4ba9-ae8f-53e8994f3f50",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "87af42834f24bb7f0213f3380346e346",
     "grade": false,
     "grade_id": "cell-a42fc6547275b356",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Excercise 2 -  Embedding Layer:\n",
    "- Implement an embedding layer from scratch. This layer should be able to:\n",
    "    - Initialize an embedding table with random values.\n",
    "    - Look up and return embeddings for a given list of indices.\n",
    "    - Handle potential out-of-bounds errors when looking up embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b0cf5a5e-1ff9-412c-9690-9e52d92b78c6",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8928f53c6da09b2c9cae21825262cec3",
     "grade": false,
     "grade_id": "cell-023c99acc0fc8c7a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "4ce9b7e1-7855-4cae-ab77-22eaec36f38c",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a8776305882434c4476c1e5e84f43c06",
     "grade": false,
     "grade_id": "cell-ce3a5539aeeb5ea9",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "revert": "class Embedding:\n    def __init__(self, n_embd: int, d_embd: int):\n        # Hint: You might use torch.randn\n        # YOUR CODE HERE\n        raise NotImplementedError()\n\n    def forward(self, input: Tensor) -> Tensor:\n        \"\"\"Perform a lookup for the given indices in the embedding table.\"\"\"\n        # YOUR CODE HERE\n        raise NotImplementedError()\n\n    def __call__(self, input: Tensor) -> Tensor:\n        # This function lets you call a class instance as a function e.g. Embedding(n_embd, d_emdb)(x)\n        # https://docs.python.org/3/reference/datamodel.html#object.__call__\n        return self.forward(input)"
   },
   "outputs": [],
   "source": [
    "class Embedding:\n",
    "    def __init__(self, n_embd: int, d_embd: int):\n",
    "        # Hint: You might use torch.randn\n",
    "        # YOUR CODE HERE\n",
    "        # raise NotImplementedError()\n",
    "        self.embedding_table = torch.randn(n_embd, d_embd)\n",
    "        self.n_embd = n_embd\n",
    "        self.d_embd = d_embd\n",
    "\n",
    "    def forward(self, input: Tensor) -> Tensor:\n",
    "        \"\"\"Perform a lookup for the given indices in the embedding table.\"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        # raise NotImplementedError()\n",
    "        return self.embedding_table[input]\n",
    "\n",
    "    def __call__(self, input: Tensor) -> Tensor:\n",
    "        # This function lets you call a class instance as a function e.g. Embedding(n_embd, d_emdb)(x)\n",
    "        # https://docs.python.org/3/reference/datamodel.html#object.__call__\n",
    "        # return self.forward(input)\n",
    "        return self.forward(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e78a282-b291-461c-88cf-dffcf80c12d7",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c17b153d5b55797c2d7dca07b819097b",
     "grade": false,
     "grade_id": "cell-615686e742f4111e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Run Exercise 2\n",
    "Run these cells to evaluate your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d604afc9-0d10-4bb1-b35e-8a0c84391d76",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6a81df52bb8705ac087a729846b0982c",
     "grade": false,
     "grade_id": "cell-fc0a933612896a75",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# A helper function to assert a function call throws an exception\n",
    "def assert_raises(fn, *args, **kwargs):\n",
    "    try:\n",
    "        fn(*args, **kwargs)\n",
    "    except Exception as e:\n",
    "        print(f\"Expected error occurred: {type(e).__name__} - {e}\")\n",
    "        return\n",
    "    raise AssertionError(\"Expected error did not occur\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "680a6a35-8615-4e2a-be40-7b5ca0032aa8",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2495aa46f6b7202ae3dc6f5fe38934c9",
     "grade": true,
     "grade_id": "cell-1aa1552ad7550a02",
     "locked": true,
     "points": 0.75,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============ Embedding Layer\n",
      "input (torch.Size([11])):\n",
      "tensor([ 2,  0, 19,  4, 17, 15,  8, 11, 11,  0, 17])\n",
      "\n",
      "embedding_layer result (torch.Size([11, 3])):\n",
      "tensor([[-1.7156, -0.9173, -1.1932],\n",
      "        [ 0.1777,  1.7889,  2.4915],\n",
      "        [ 0.4360, -0.7645,  0.0196],\n",
      "        [ 0.5832, -0.5796,  0.8302],\n",
      "        [-0.3977,  0.8353, -0.5595],\n",
      "        [ 0.5928,  0.5769,  1.1962],\n",
      "        [-0.4689,  0.4957,  0.2500],\n",
      "        [-0.5763,  0.5295, -0.3352],\n",
      "        [-0.5763,  0.5295, -0.3352],\n",
      "        [ 0.1777,  1.7889,  2.4915],\n",
      "        [-0.3977,  0.8353, -0.5595]])\n",
      "Expected error occurred: IndexError - index 41 is out of bounds for dimension 0 with size 41\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Expected Output:\n",
    "\n",
    "============ Embedding Layer\n",
    "input (torch.Size([11])):\n",
    "tensor([28, 26, 45, 30, 43, 41, 34, 37, 37, 26, 43]) # these numbers will be different\n",
    "\n",
    "embedding_layer result (torch.Size([11, 3])):  # these numbers will be different, depending on your vocab size\n",
    "tensor([[-0.0748,  0.5664, -0.6240], # all of the following numbers will be different and also different per run\n",
    "        [-1.9658, -0.7646, -0.4583],\n",
    "        [ 1.1624,  0.8075, -0.5995],\n",
    "        [ 0.4513, -0.0109,  0.2278],\n",
    "        [-1.2602, -0.8705, -0.0846],\n",
    "        [ 0.3563,  0.4905,  0.5740],\n",
    "        [ 0.5596,  0.3183, -2.2232],\n",
    "        [-0.2117, -0.0676,  1.6243],\n",
    "        [-0.2117, -0.0676,  1.6243],\n",
    "        [-1.9658, -0.7646, -0.4583],\n",
    "        [-1.2602, -0.8705, -0.0846]])\n",
    "Expected error occurred: ValueError - Input tensor contains invalid indices for lookup table. # The string will depend on which error you throw\n",
    "\"\"\"\n",
    "print(\"============ Embedding Layer\")\n",
    "n_embd = len(vocab)\n",
    "d_embd = 3\n",
    "embedding_layer = Embedding(n_embd, d_embd)\n",
    "\n",
    "input_tensor = torch.tensor(tokenizer.encode(tokenizer.parse('caterpillar')))\n",
    "result = embedding_layer(input_tensor)\n",
    "\n",
    "print(f\"input ({input_tensor.size()}):\\n{input_tensor}\\n\")\n",
    "print(f\"embedding_layer result ({result.size()}):\\n{result}\")\n",
    "\n",
    "# Assure layer throws exception on invalid index\n",
    "assert_raises(embedding_layer, torch.tensor([n_embd]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba0b7db-c3d3-47d4-835c-0b2072b212c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python with Torch",
   "language": "python",
   "name": "python3_2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
