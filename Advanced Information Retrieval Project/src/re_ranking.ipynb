{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D8usSW9Bwv4h"
   },
   "source": [
    "# AIR - Exercise in Google Colab\n",
    "\n",
    "## Colab Preparation\n",
    "\n",
    "Open via google drive -> right click: open with Colab\n",
    "\n",
    "**Get a GPU**\n",
    "\n",
    "Toolbar -> Runtime -> Change Runtime Type -> GPU\n",
    "\n",
    "**Mount Google Drive**\n",
    "\n",
    "* Download data and clone your github repo to your Google Drive folder\n",
    "* Use Google Drive as connection between Github and Colab (Could also use direct github access, but re-submitting credentials might be annoying)\n",
    "* Commit to Github locally from the synced drive\n",
    "\n",
    "**Keep Alive**\n",
    "\n",
    "When training google colab tends to kick you out, This might help: https://medium.com/@shivamrawat_756/how-to-prevent-google-colab-from-disconnecting-717b88a128c0\n",
    "\n",
    "**Get Started**\n",
    "\n",
    "Run the following script to mount google drive and install needed python packages. Pytorch comes pre-installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Sfiw_6jZ0uWa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfrom google.colab import drive\\ndrive.mount('/content/drive')\\n\\n!pip install -r ../requirements.txt\\n\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "!pip install -r ../requirements.txt\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "IUVVDw1m2sed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version: 1.7.1\n",
      "Has GPU: True\n",
      "Random tensor: tensor([0.9229, 0.9077, 0.5947, 0.5328, 0.0282, 0.4172, 0.2058, 0.4862, 0.8398,\n",
      "        0.3297], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(\"Version:\",torch.__version__)\n",
    "print(\"Has GPU:\",torch.cuda.is_available()) # check that 1 gpu is available\n",
    "print(\"Random tensor:\",torch.rand(10,device=\"cuda\")) # check that pytorch works "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fvQMmxs0x_x8"
   },
   "source": [
    "# Main.py Replacement\n",
    "\n",
    "-> add your code here\n",
    "\n",
    "- Replace *air_test* with your google drive location in the sys.path.append()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "Y_IEUP_2-099"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "from allennlp.common import Params, Tqdm\n",
    "from allennlp.common.util import prepare_environment\n",
    "from allennlp.data.dataloader import PyTorchDataLoader\n",
    "prepare_environment(Params({})) # sets the seeds to be fixed\n",
    "\n",
    "import torch\n",
    "\n",
    "from allennlp.data.vocabulary import Vocabulary\n",
    "\n",
    "from allennlp.modules.token_embedders import Embedding\n",
    "from allennlp.modules.text_field_embedders import BasicTextFieldEmbedder\n",
    "from allennlp.nn.util import move_to_device\n",
    "\n",
    "from data_loading import *\n",
    "from model_knrm import *\n",
    "from model_tk import *\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from torch.optim import Adam\n",
    "\n",
    "from core_metrics import (\n",
    "    unrolled_to_ranked_result,\n",
    "    load_qrels,\n",
    "    calculate_metrics_plain\n",
    ")\n",
    "\n",
    "# importlib.import_module('model_tk')\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "# change paths to your data directory\n",
    "config = {\n",
    "    \"vocab_directory\": \"../Part-2/allen_vocab_lower_10\",\n",
    "    \"pre_trained_embedding\": \"../Part-2/glove.42B.300d.txt\",\n",
    "    \"model\": \"knrm\",\n",
    "    \"train_data\": \"../Part-2/triples.train.tsv\",\n",
    "    \"validation_data\": \"../Part-2/msmarco_tuples.validation.tsv\",\n",
    "    \"test_data\": \"../Part-2/msmarco_tuples.test.tsv\",\n",
    "    \n",
    "    \"qrels\" : \"../Part-2/msmarco_qrels.txt\",\n",
    "\n",
    "    # Fira datasets\n",
    "    'fira_2022_data_set': \"../Part-2/fira-22.tuples.tsv\",\n",
    "    'fira_2022_baseline_qrels': \"../Part-2/fira-22.baseline-qrels.tsv\",\n",
    "    'fira_2022_PART_1': \"../Part-2/Final_Exercise_1.tsv\",\n",
    "\n",
    "    \"validation_frequency\": 200,\n",
    "\n",
    "    # Part 3 data sets\n",
    "    \"msmarco_fira_21_qa_answers\": \"../Part-3/msmarco-fira-21.qrels.qa-answers.tsv\",\n",
    "    \"msmarco_fira_21_qa_tuples\": \"../Part-3/msmarco-fira-21.qrels.qa-tuples.tsv\"\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading is starting...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b293ae87704349609a6665f2adaae8d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loading has been completed!\n",
      "Wall time: 29.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#\n",
    "# data loading\n",
    "#\n",
    "print(\"Loading is starting...\")\n",
    "vocab = Vocabulary.from_files(config[\"vocab_directory\"])\n",
    "tokens_embedder = Embedding(vocab=vocab,\n",
    "                           pretrained_file= config[\"pre_trained_embedding\"],\n",
    "                           embedding_dim=300,\n",
    "                           trainable=True,\n",
    "                           padding_index=0)\n",
    "word_embedder = BasicTextFieldEmbedder({\"tokens\": tokens_embedder})\n",
    "print(\"The loading has been completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model knrm total parameters: 94382412\n",
      "Network: KNRM(\n",
      "  (word_embeddings): BasicTextFieldEmbedder(\n",
      "    (token_embedder_tokens): Embedding()\n",
      "  )\n",
      "  (cosine_module): CosineMatrixAttention()\n",
      "  (dense): Linear(in_features=11, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# recommended default params for the models (but you may change them if you want)\n",
    "if config[\"model\"] == \"knrm\":\n",
    "    model = KNRM(word_embedder, n_kernels=11)\n",
    "elif config[\"model\"] == \"tk\":\n",
    "    model = TK(word_embedder, n_kernels=11, n_layers = 2, n_tf_dim = 300, n_tf_heads = 10)\n",
    "\n",
    "\n",
    "# todo optimizer, loss \n",
    "loss_function = torch.nn.MarginRankingLoss(margin=1, reduction=\"mean\").to(device)\n",
    "optimizer = Adam(model.parameters(), lr=1e-4, weight_decay=0.001)\n",
    "\n",
    "print('Model',config[\"model\"],'total parameters:', sum(p.numel() for p in model.parameters() if p.requires_grad))\n",
    "print('Network:', model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load Training Data\n",
      "DONE!\n"
     ]
    }
   ],
   "source": [
    "print(\"Load Training Data\")\n",
    "_triple_reader = IrTripleDatasetReader(lazy=True, max_doc_length=180, max_query_length=30)\n",
    "_triple_reader = _triple_reader.read(config[\"train_data\"])\n",
    "_triple_reader.index_with(vocab)\n",
    "train_loader = PyTorchDataLoader(_triple_reader, batch_size=32)\n",
    "print(\"DONE!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load Validation Data\n",
      "DONE!\n"
     ]
    }
   ],
   "source": [
    "print(\"Load Validation Data\")\n",
    "_val_reader = IrLabeledTupleDatasetReader(\n",
    "        lazy=True, max_doc_length=180, max_query_length=30\n",
    "    )\n",
    "_val_reader = _val_reader.read(config[\"validation_data\"])\n",
    "_val_reader.index_with(vocab)\n",
    "val_loader = PyTorchDataLoader(_val_reader, batch_size=128)\n",
    "print(\"DONE!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the evaluation function in order to evaluate our models based on the corresponding data set\n",
    "def evaluate_model(model, data_loader, p, Part_3 = False):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    all_scores = []\n",
    "\n",
    "    print(\"Start Testing\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in Tqdm.tqdm(data_loader):\n",
    "            batch = move_to_device(batch, device)\n",
    "            queries, docs = batch[\"query_tokens\"], batch[\"doc_tokens\"]\n",
    "            scores = model(queries, docs).squeeze().tolist()\n",
    "            ids_scores = zip(batch[\"query_id\"], batch[\"doc_id\"], scores)\n",
    "            all_scores.extend(ids_scores)\n",
    "\n",
    "    print(\"End Testing\")\n",
    "    print(\"===========\")\n",
    "\n",
    "    if Part_3:\n",
    "        return all_scores\n",
    "\n",
    "    print(\"Start Evaluation\")\n",
    "\n",
    "    # Process results\n",
    "    results = {}\n",
    "    for q_id, d_id, score in all_scores:\n",
    "        if q_id not in results:\n",
    "            results[q_id] = []\n",
    "        results[q_id].append((d_id, score))\n",
    "\n",
    "    # Convert to ranked results and calculate metrics\n",
    "    ranked_results = unrolled_to_ranked_result(results)\n",
    "\n",
    "    qrels = load_qrels(p)\n",
    "    \n",
    "    metrics = calculate_metrics_plain(ranked_results, qrels)\n",
    "    metrics_list = [metrics[k] for k in [\"MRR@10\", \"MRR@20\", \"nDCG@10\", \"nDCG@20\"]]\n",
    "    rounded_metrics = tuple(round(m, 3) for m in metrics_list)\n",
    "\n",
    "    print(\"End Evaluation\")\n",
    "\n",
    "    return rounded_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training Loop\n",
      "Starting epoch....\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3164649419f6470cb4a47e041701177f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5adb5fb13f5346c38c7a1023879b8407",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "reading instances: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Testing\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee4f3bf4b01b417b8d762e97951acbbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29d5d77139dc4ee8aaa6072c45c5475d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "reading instances: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End Testing\n",
      "===========\n",
      "Start Evaluation\n",
      "End Evaluation\n",
      "Validation Results - Epoch: 1, MRR@10: 0.095, MRR@20: 0.104, nDCG@10: 0.128, nDCG@20: 0.162\n",
      "Validations Runs:  0\n",
      "Start Testing\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4aa437b5d924c04b45b40a976d1ebd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e697dce95054a31856d0a319d2aefe1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "reading instances: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End Testing\n",
      "===========\n",
      "Start Evaluation\n",
      "End Evaluation\n",
      "Validation Results - Epoch: 1, MRR@10: 0.146, MRR@20: 0.154, nDCG@10: 0.181, nDCG@20: 0.214\n",
      "Validations Runs:  0\n",
      "Start Testing\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de7aef7e41a2472bb6a3bb8dda627aca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "860bf28ebe4b44c9a26cd00bfee4a44a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "reading instances: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End Testing\n",
      "===========\n",
      "Start Evaluation\n",
      "End Evaluation\n",
      "Validation Results - Epoch: 1, MRR@10: 0.144, MRR@20: 0.153, nDCG@10: 0.182, nDCG@20: 0.213\n",
      "Validations Runs:  1\n",
      "Start Testing\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "350405c470df4b50975955323404afd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf071444a59f43158b3ea2e81f8ac2f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "reading instances: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End Testing\n",
      "===========\n",
      "Start Evaluation\n",
      "End Evaluation\n",
      "Validation Results - Epoch: 1, MRR@10: 0.146, MRR@20: 0.154, nDCG@10: 0.183, nDCG@20: 0.214\n",
      "Validations Runs:  2\n",
      "Start Testing\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30c82a9d228f4bd9bfde3a364875c965",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3e408887a664ded8deb714035007329",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "reading instances: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End Testing\n",
      "===========\n",
      "Start Evaluation\n",
      "End Evaluation\n",
      "Validation Results - Epoch: 1, MRR@10: 0.146, MRR@20: 0.154, nDCG@10: 0.183, nDCG@20: 0.214\n",
      "Validations Runs:  3\n",
      "Start Testing\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cf8e352f16a4866938cf6af08a5b29f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59ac8fed45db45098b483a7984340dd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "reading instances: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End Testing\n",
      "===========\n",
      "Start Evaluation\n",
      "End Evaluation\n",
      "Validation Results - Epoch: 1, MRR@10: 0.142, MRR@20: 0.151, nDCG@10: 0.179, nDCG@20: 0.211\n",
      "Validations Runs:  4\n",
      "Start Testing\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "598804fb9659419888a876a453e87579",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b2795aa2a424d11a42824354a91e762",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "reading instances: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End Testing\n",
      "===========\n",
      "Start Evaluation\n",
      "End Evaluation\n",
      "Validation Results - Epoch: 1, MRR@10: 0.141, MRR@20: 0.15, nDCG@10: 0.177, nDCG@20: 0.211\n",
      "Validations Runs:  5\n",
      "Early stopping, since MRR@10 did not improve for 5 validation runs.\n",
      "Epoch Completed!\n",
      "Epoch Completed!\n",
      "Starting epoch....\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87685a43727b429891eb87cbda0b7772",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e24e3d49a8f47299d9740f3963bbc11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "reading instances: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Testing\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98ac1267dc064740a621bc162719e946",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d07707cace1748b2a09d635b40e0df52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "reading instances: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End Testing\n",
      "===========\n",
      "Start Evaluation\n",
      "End Evaluation\n",
      "Validation Results - Epoch: 2, MRR@10: 0.141, MRR@20: 0.15, nDCG@10: 0.177, nDCG@20: 0.21\n",
      "Validations Runs:  1\n",
      "Start Testing\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75be7baa15c643298dd9f361e310f593",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "077e92a58bc54fdf92dd9a20f2f4f85b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "reading instances: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End Testing\n",
      "===========\n",
      "Start Evaluation\n",
      "End Evaluation\n",
      "Validation Results - Epoch: 2, MRR@10: 0.138, MRR@20: 0.147, nDCG@10: 0.174, nDCG@20: 0.208\n",
      "Validations Runs:  2\n",
      "Start Testing\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0819846dbe7149cdaf923db81ba20873",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06cc2c6a21034036b07f443617b54684",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "reading instances: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End Testing\n",
      "===========\n",
      "Start Evaluation\n",
      "End Evaluation\n",
      "Validation Results - Epoch: 2, MRR@10: 0.136, MRR@20: 0.145, nDCG@10: 0.172, nDCG@20: 0.206\n",
      "Validations Runs:  3\n",
      "Start Testing\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a94a7bb610d42429ccb8400d2017e75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79c5f142c4314f87acf4a057b5f2ba3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "reading instances: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End Testing\n",
      "===========\n",
      "Start Evaluation\n",
      "End Evaluation\n",
      "Validation Results - Epoch: 2, MRR@10: 0.136, MRR@20: 0.145, nDCG@10: 0.172, nDCG@20: 0.205\n",
      "Validations Runs:  4\n",
      "Start Testing\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebda2de3d67a4d0eabf0209b407a0f2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa7f9e52306647b9821cbae9ec9756b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "reading instances: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End Testing\n",
      "===========\n",
      "Start Evaluation\n",
      "End Evaluation\n",
      "Validation Results - Epoch: 2, MRR@10: 0.136, MRR@20: 0.145, nDCG@10: 0.172, nDCG@20: 0.205\n",
      "Validations Runs:  5\n",
      "Early stopping, since MRR@10 did not improve for 5 validation runs.\n",
      "Epoch Completed!\n",
      "Epoch Completed!\n",
      "Starting epoch....\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f7321974834429cadd7194a53f1f20a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ca6c4f2fc304016a64f03ee7cdccc9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "reading instances: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Testing\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fe153e11be948938e9d05706e4e64d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "859437c9020a4575baf6ef709b93864e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "reading instances: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End Testing\n",
      "===========\n",
      "Start Evaluation\n",
      "End Evaluation\n",
      "Validation Results - Epoch: 3, MRR@10: 0.136, MRR@20: 0.145, nDCG@10: 0.172, nDCG@20: 0.205\n",
      "Validations Runs:  1\n",
      "Start Testing\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "223ae0c9901e4387ad424388c5eec2e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8df38a002484d89a177eca3ad01601d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "reading instances: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End Testing\n",
      "===========\n",
      "Start Evaluation\n",
      "End Evaluation\n",
      "Validation Results - Epoch: 3, MRR@10: 0.133, MRR@20: 0.142, nDCG@10: 0.169, nDCG@20: 0.202\n",
      "Validations Runs:  2\n",
      "Start Testing\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bb2c1a4f4e74e058af6646040a83642",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd3891d0ec9b4282aa2411428a6cef0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "reading instances: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End Testing\n",
      "===========\n",
      "Start Evaluation\n",
      "End Evaluation\n",
      "Validation Results - Epoch: 3, MRR@10: 0.131, MRR@20: 0.14, nDCG@10: 0.167, nDCG@20: 0.201\n",
      "Validations Runs:  3\n",
      "Start Testing\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5814b1fd829f43e9acd08dc2a76d6d2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f2a5e1abf5c475881785b9bbfc381ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "reading instances: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End Testing\n",
      "===========\n",
      "Start Evaluation\n",
      "End Evaluation\n",
      "Validation Results - Epoch: 3, MRR@10: 0.132, MRR@20: 0.141, nDCG@10: 0.167, nDCG@20: 0.201\n",
      "Validations Runs:  4\n",
      "Start Testing\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f846c4259f9d42d78f0415864753d79f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b0f747769f94cd7b376f5b142049357",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "reading instances: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End Testing\n",
      "===========\n",
      "Start Evaluation\n",
      "End Evaluation\n",
      "Validation Results - Epoch: 3, MRR@10: 0.131, MRR@20: 0.14, nDCG@10: 0.167, nDCG@20: 0.201\n",
      "Validations Runs:  5\n",
      "Early stopping, since MRR@10 did not improve for 5 validation runs.\n",
      "Epoch Completed!\n",
      "Epoch Completed!\n",
      "END OF TRAINING LOOP\n",
      "Wall time: 14min 11s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(\"Start training Loop\")\n",
    "results = []\n",
    "\n",
    "best = 0\n",
    "last = 0\n",
    "num_epochs = 3\n",
    "for epoch in range(num_epochs):\n",
    "    print(\"Starting epoch....\")\n",
    "    losses = []\n",
    "    last = 0\n",
    "    iteration = 0\n",
    "    for batch in Tqdm.tqdm(train_loader):\n",
    "        model.to(device)\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        batch = move_to_device(batch, device)\n",
    "\n",
    "        # Positive and negative scores\n",
    "\n",
    "        positive_scores = model.forward(batch[\"query_tokens\"], batch[\"doc_pos_tokens\"])\n",
    "        negative_scores = model.forward(batch[\"query_tokens\"], batch[\"doc_neg_tokens\"]) \n",
    "\n",
    "        # Labels for MarginRankingLoss should be 1 for positive, -1 for negative\n",
    "        target = torch.ones(batch[\"query_tokens\"][\"tokens\"][\"tokens\"].shape[0], dtype=torch.float).to(device)\n",
    "\n",
    "        # Compute loss\n",
    "        current_loss = loss_function(positive_scores, negative_scores, target)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        current_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        losses.append(current_loss.item())\n",
    "\n",
    "        # Validation evaluation\n",
    "        if iteration % config[\"validation_frequency\"] == 0:\n",
    "            mrr10, mrr20, ndcg10, ndcg20 = evaluate_model(model, val_loader, config['qrels'])\n",
    "            print(f\"Validation Results - Epoch: {epoch + 1}, MRR@10: {mrr10}, MRR@20: {mrr20}, nDCG@10: {ndcg10}, nDCG@20: {ndcg20}\")\n",
    "\n",
    "            if mrr10 > best:\n",
    "                    best = mrr10\n",
    "                    last = 0\n",
    "            else:\n",
    "                last += 1\n",
    "            print('Validations Runs: ',last)\n",
    "            if last >= 5 and best != 0:\n",
    "                print(\"Early stopping, since MRR@10 did not improve for 5 validation runs.\")\n",
    "                break  \n",
    "        iteration += 1\n",
    "\n",
    "    print(\"Epoch Completed!\")\n",
    "\n",
    "    # avg_train_loss = sum(losses) / iteration\n",
    "    # print(f\"Epoch: {epoch + 1}, Average Training Loss: {avg_train_loss:.4f}\")\n",
    "    print(\"Epoch Completed!\")  \n",
    "print(\"END OF TRAINING LOOP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved successfully.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "The following code block is not related to the training, evaluation, and testing of our models. \n",
    "However, the reason this code is used is either to save our models in .pth file format or to load \n",
    "the already saved models so that we do not have to retrain our models.\n",
    "\"\"\"\n",
    "# Save or Load the model\n",
    "if config[\"model\"] == \"knrm\":\n",
    "    model_file_path = 'ModelKNRM.pth'\n",
    "elif config[\"model\"] == \"tk\":\n",
    "    model_file_path = 'ModelTK.pth'\n",
    "\n",
    "# Check if the model file exists\n",
    "if os.path.exists(model_file_path):\n",
    "    # Load the model\n",
    "    model.load_state_dict(torch.load(model_file_path)) \n",
    "    print(\"Model loaded successfully.\")\n",
    "else:\n",
    "    # Save the model\n",
    "    torch.save(model.state_dict(), model_file_path)\n",
    "    print(\"Model saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Testing\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72efe87b58034e968521daf4948bb62d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db0d9428e6804aa788095ed3dad9ee7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "reading instances: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End Testing\n",
      "===========\n",
      "Start Evaluation\n",
      "End Evaluation\n",
      "Testing Results - MRR@10: 0.131, MRR@20: 0.141, nDCG@10: 0.169, nDCG@20: 0.205\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Use the msmarco_tuples.test.tsv input to feed the neural models and msmarco_qrels.txt qrels to evaluate the output\n",
    "'''\n",
    "\n",
    "# msmarco_tuples.test.tsv\n",
    "# msmarco_qrels.txt\n",
    "_test_reader = IrLabeledTupleDatasetReader(lazy=True, max_doc_length=180, max_query_length=30)\n",
    "_test_reader = _test_reader.read(config[\"test_data\"])\n",
    "_test_reader.index_with(vocab)\n",
    "test_loader = PyTorchDataLoader(_test_reader, batch_size=128)\n",
    "\n",
    "mrr10, mrr20, ndcg10, ndcg20 = evaluate_model(model, test_loader, config['qrels'])\n",
    "print(f\"Testing Results - MRR@10: {mrr10}, MRR@20: {mrr20}, nDCG@10: {ndcg10}, nDCG@20: {ndcg20}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Testing\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f21d35728a8440d29abbe819b6feecc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c538c546ad9a4459b830760bc1e72568",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "reading instances: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End Testing\n",
      "===========\n",
      "Start Evaluation\n",
      "End Evaluation\n",
      "Testing Results - MRR@10: 0.95, MRR@20: 0.95, nDCG@10: 0.894, nDCG@20: 0.908\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Use the fira-2022.tuples.tsv input to feed the neural models and fira-2022.baseline-qrels.tsv qrels to evaluate the output\n",
    "'''\n",
    "\n",
    "# fira-2022.tuples.tsv\n",
    "# fira-2022.baseline-qrels.tsv\n",
    "\n",
    "_tuple_reader = IrLabeledTupleDatasetReader(lazy=True, max_doc_length=180, max_query_length=30)\n",
    "_tuple_reader = _tuple_reader.read(config[\"fira_2022_data_set\"])\n",
    "_tuple_reader.index_with(vocab)\n",
    "fira_baseline_loader = PyTorchDataLoader(_tuple_reader, batch_size=128)\n",
    "\n",
    "mrr10, mrr20, ndcg10, ndcg20 = evaluate_model(model, fira_baseline_loader, config['fira_2022_baseline_qrels'])\n",
    "print(f\"Testing Results - MRR@10: {mrr10}, MRR@20: {mrr20}, nDCG@10: {ndcg10}, nDCG@20: {ndcg20}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Testing\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8be7a4893634f6f85d21864dc6d5c89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "989359bc982c41569514625b5b34f55a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "reading instances: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End Testing\n",
      "===========\n",
      "Start Evaluation\n",
      "End Evaluation\n",
      "Testing Results - MRR@10: 0.727, MRR@20: 0.727, nDCG@10: 0.746, nDCG@20: 0.767\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Use the fira-2022.tuples.tsv input to feed the neural models and your qrels from part 1 to evaluate the output\n",
    "'''\n",
    "\n",
    "# fira-2022.tuples.tsv\n",
    "# Final_Exercise_1.tsv\n",
    "\n",
    "_tuple_reader = IrLabeledTupleDatasetReader(lazy=True, max_doc_length=180, max_query_length=30)\n",
    "_tuple_reader = _tuple_reader.read(config[\"fira_2022_data_set\"])\n",
    "_tuple_reader.index_with(vocab)\n",
    "fira_Part1_loader = PyTorchDataLoader(_tuple_reader, batch_size=128)\n",
    "\n",
    "mrr10, mrr20, ndcg10, ndcg20 = evaluate_model(model, fira_Part1_loader, config['fira_2022_PART_1'])\n",
    "print(f\"Testing Results - MRR@10: {mrr10}, MRR@20: {mrr20}, nDCG@10: {ndcg10}, nDCG@20: {ndcg20}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here the last bullet for part 3 starts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The following code block is not related to the training, evaluation, and testing of our models.\n",
    "\"\"\"\n",
    "# Start the last bullet for part 3 using the msmarco test data set\n",
    "# Our goal is to create a top-1 MSMARCO passage results from the best re-ranking model\n",
    "# Then we take this top-1 data set (Format: query_id    doc_id\t    question\t    answer)\n",
    "# and use the pre trained model in order to extract the relevant information\n",
    "# Finally this is relevance information is evaluated using the \n",
    "# 1. msmarco-fira-21.qrels.qa-answers.tsv \n",
    "# 2. msmarco-fira-21.qrels.qa-tuples.tsv\n",
    "\n",
    "# msmarco_tuples.test.tsv\n",
    "# msmarco_qrels.txt\n",
    "_test_reader = IrLabeledTupleDatasetReader(lazy=True, max_doc_length=180, max_query_length=30)\n",
    "_test_reader = _test_reader.read(config[\"test_data\"])\n",
    "_test_reader.index_with(vocab)\n",
    "test_loader = PyTorchDataLoader(_test_reader, batch_size=128)\n",
    "\n",
    "##############################################################################################################\n",
    "# Retrieve all the score for each (query id-document id) pair\n",
    "all_scores = evaluate_model(model, test_loader, config['qrels'], True)\n",
    "# Change it to data frame\n",
    "all_scores_pd = pd.DataFrame(all_scores, columns=['query_id', 'doc_id', 'score'])\n",
    "# Get the top-1 MSMARCO passage results from the best re-ranking model (last bullet for part 3 - Extractive QA)\n",
    "top_1 = all_scores_pd.loc[all_scores_pd.groupby('query_id')['score'].idxmax()].reset_index(drop=True).drop(columns = ['score'])\n",
    "\n",
    "\n",
    "column_names = ['query_id', 'doc_id', 'question', 'answer']\n",
    "# Read the .tsv file with the specified column names\n",
    "msmarco_test = pd.read_csv(config['test_data'], sep='\\t', names=column_names)\n",
    "\n",
    "# Change the query and doc id columns to int\n",
    "top_1['query_id'] = top_1['query_id'].astype(int)\n",
    "top_1['doc_id'] = top_1['doc_id'].astype(int)\n",
    "\n",
    "# Final merge (using inner join) between the msmarco_test data set and the top-1\n",
    "# The goal of merging is to have the combination of (query_id doc_id) with the \n",
    "# highest score (based on out re-ranking model (here is the tk model)) \n",
    "# and also the question and answer, based on the (query_id doc_id)\n",
    "final_top_1 = pd.merge(msmarco_test, top_1, on=['query_id', 'doc_id'], how='inner')\n",
    "\n",
    "# Save the data set for the part 3\n",
    "filename_dataset = \"../Part-2/\" + config['model'] + \"_dataset_for_part3.tsv\"\n",
    "final_top_1.to_csv(filename_dataset, sep='\\t', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
