{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extrative Question Answering\n",
    "\n",
    "From lecture slides\n",
    "Given a query and a passage/document: Select the words in the passage that answer the query\n",
    "\n",
    "Having a passage guaranteed to contain the answer is somewhat artificial\n",
    "\n",
    "TASK: run the extractive QA pipeline on the top-1 neural re-ranking result of the MSMARCO FIRA + on the gold-label pairs of MSMARCO-FiRA-2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IUVVDw1m2sed"
   },
   "outputs": [],
   "source": [
    "## implement part 3 here \n",
    "from core_metrics import compute_f1, compute_exact\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "from transformers import pipeline\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\a96b3nc\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# https://huggingface.co/deepset/tinyroberta-squad2\n",
    "model_name = \"deepset/tinyroberta-squad2\"\n",
    "eqa_model = pipeline('question-answering', model=model_name, tokenizer=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(filename, reranked=False):\n",
    "    rows = []\n",
    "    with open(filename, \"r\", encoding=\"utf-8\") as file:\n",
    "        for line in file:\n",
    "            sep = line.split(\"\\t\")\n",
    "            if reranked:\n",
    "                answers = [element.replace('\\n', '') for element in sep[4:]] # handles variable ground truth amounts\n",
    "                row = {'queryid':sep[0], \n",
    "                    'documentid':sep[1], \n",
    "                    'relevance-grade':sep[2], \n",
    "                    'text-selection':answers\n",
    "                }\n",
    "            else:\n",
    "                answers = [element.replace('\\n', '') for element in sep[6:]] # handles variable ground truth amounts\n",
    "                row = {'queryid':sep[0], \n",
    "                        'documentid':sep[1], \n",
    "                        'relevance-grade':sep[2], \n",
    "                        'question':sep[3], \n",
    "                        'context':sep[4], \n",
    "                        'text-selection':answers\n",
    "                    }\n",
    "            rows.append(row)\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def prep_input(qatuples):\n",
    "    '''\n",
    "    Prepares tuples into dictionaries containing questions and contexts, a suitable format for the extractive qa model.\n",
    "    '''\n",
    "    return qatuples[['question', 'context']].to_dict(orient='records')\n",
    "\n",
    "def gen_answer(inputs):\n",
    "    '''\n",
    "    Computes a list of answers for given questions and contexts (qa tuples).\n",
    "    '''\n",
    "    return [eqa_model(i) for i in tqdm(inputs)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### gold-label pairs of MSMARCO-FiRA-2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 52606/52606 [1:59:46<00:00,  7.32it/s]\n"
     ]
    }
   ],
   "source": [
    "input = read_file('Part-3/msmarco-fira-21.qrels.qa-tuples.tsv')\n",
    "model_input = prep_input(input)\n",
    "answer_pred = gen_answer(model_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_pred_vec = [a['answer'] for a in answer_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score\n",
      "MEAN:  0.3710\n",
      "STD:  0.3063\n"
     ]
    }
   ],
   "source": [
    "# Computing f-statistics with gold-label pairs\n",
    "f1_tuples = []\n",
    "for i in range(len(input)):\n",
    "    res = compute_f1(str(input.loc[i, 'text-selection']), answer_pred_vec[i])\n",
    "    f1_tuples.append(res)\n",
    "\n",
    "print(\"f1 score\")\n",
    "print(f\"MEAN:  {np.mean(f1_tuples):.4f}\")\n",
    "print(f\"STD:  {np.std(f1_tuples):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact score\n",
      "MEAN:  0.0934\n",
      "STD:  0.2910\n"
     ]
    }
   ],
   "source": [
    "# Computing exact matching with gold-label pairs\n",
    "compute_exact_tuples = []\n",
    "for i in range(len(input)):\n",
    "    res = compute_exact(str(input.loc[i, 'text-selection']), answer_pred_vec[i])\n",
    "    compute_exact_tuples.append(res)\n",
    "\n",
    "print(\"Exact score\")\n",
    "print(f\"MEAN:  {np.mean(compute_exact_tuples):.4f}\")\n",
    "print(f\"STD:  {np.std(compute_exact_tuples):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = read_file('Part-3/msmarco-fira-21.qrels.qa-answers.tsv', reranked=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use here our top-1 MSMARCO passage results from the best re-ranking model\n",
    "\n",
    "only evaluate the overlap of pairs that are in the result and the qrels, hence below data from part 2 and answers are merged. Only 889 from 2000 rows remain, i.e., 1111 have no reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = pd.read_csv('../Project_Part_3/tk_dataset_for_part3.tsv', delimiter='\\t', header=None, names=['queryid', 'documentid', 'question', 'context'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer['queryid'] = answer['queryid'].astype('int64')\n",
    "answer['documentid'] = answer['documentid'].astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = input.merge(answer, on=['queryid', 'documentid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 889/889 [02:31<00:00,  5.86it/s]\n"
     ]
    }
   ],
   "source": [
    "model_input = prep_input(input)\n",
    "answer_pred = gen_answer(model_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_pred_vec = [a['answer'] for a in answer_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score\n",
      "MEAN:  0.4180\n",
      "STD:  0.3087\n"
     ]
    }
   ],
   "source": [
    "# Computing f-statistics with gold-label pairs\n",
    "f1_tuples = []\n",
    "for i in range(len(input)):\n",
    "    res = compute_f1(str(input.loc[i, 'text-selection']), answer_pred_vec[i])\n",
    "    f1_tuples.append(res)\n",
    "\n",
    "print(\"f1 score\")\n",
    "print(f\"MEAN:  {np.mean(f1_tuples):.4f}\")\n",
    "print(f\"STD:  {np.std(f1_tuples):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact score\n",
      "MEAN:  0.1125\n",
      "STD:  0.3160\n"
     ]
    }
   ],
   "source": [
    "# Computing exact matching with gold-label pairs\n",
    "compute_exact_tuples = []\n",
    "for i in range(len(input)):\n",
    "    res = compute_exact(str(input.loc[i, 'text-selection']), answer_pred_vec[i])\n",
    "    compute_exact_tuples.append(res)\n",
    "\n",
    "print(\"Exact score\")\n",
    "print(f\"MEAN:  {np.mean(compute_exact_tuples):.4f}\")\n",
    "print(f\"STD:  {np.std(compute_exact_tuples):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "train.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
